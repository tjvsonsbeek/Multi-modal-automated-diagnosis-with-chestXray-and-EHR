# Automated diagnosis With multi-modal Learning Using Electronic Health Records and ChestX-rays

## Abstract

Deep learning has led to state-of-the-art results for many medical imaging tasks, such as segmentation and classification of different anatomical structures. Most of these applications focus on the images alone ignoring the wealth of information contained in the Electronic Health Records these images are part of. Jointly learning from EHR and medical images, thus using more data, has great potential.  
In this paper we propose a fully automated multi-modal model which creates a joint context-aware feature representation based on prior patient information from the EHR and an associated X-ray scan. This feature representation, which joins the two different modalities through attention, leverages the contextual relationship between the two modalities. This model is used to perform two tasks: diagnosis classification and free-text diagnosis generation. We show the benefit of the multi-modal approach over single-modality approaches and outline the future challenges in this domain.

